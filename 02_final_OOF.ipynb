{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aldQtXhkCeVR"
   },
   "source": [
    "# OOF(Out of Fold) Prediction\n",
    "- Get the verage the prediction probabilities of each fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 75
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1230,
     "status": "ok",
     "timestamp": 1599725019377,
     "user": {
      "displayName": "권철민",
      "photoUrl": "",
      "userId": "03917677622451543916"
     },
     "user_tz": -540
    },
    "id": "C09IrTNHCeVX",
    "outputId": "9fa884e3-3a0b-47dc-baab-8dcb9a09d9fb"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "pd.set_option('display.max_rows', 300)\n",
    "pd.set_option('display.max_columns', 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "exeQaOvKCeVp"
   },
   "source": [
    "# Load all dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1662,
     "status": "ok",
     "timestamp": 1599725316475,
     "user": {
      "displayName": "권철민",
      "photoUrl": "",
      "userId": "03917677622451543916"
     },
     "user_tz": -540
    },
    "id": "RBOWiVA-CeVt"
   },
   "outputs": [],
   "source": [
    "def get_dataset():\n",
    "    prev_dtype = {\n",
    "        'SK_ID_PREV':np.uint32, 'SK_ID_CURR':np.uint32, 'HOUR_APPR_PROCESS_START':np.int32, 'NFLAG_LAST_APPL_IN_DAY':np.int32,\n",
    "        'DAYS_DECISION':np.int32, 'SELLERPLACE_AREA':np.int32, 'AMT_ANNUITY':np.float32, 'AMT_APPLICATION':np.float32,\n",
    "        'AMT_CREDIT':np.float32, 'AMT_DOWN_PAYMENT':np.float32, 'AMT_GOODS_PRICE':np.float32, 'RATE_DOWN_PAYMENT':np.float32,\n",
    "        'RATE_INTEREST_PRIMARY':np.float32, 'RATE_INTEREST_PRIVILEGED':np.float32, 'CNT_PAYMENT':np.float32,\n",
    "        'DAYS_FIRST_DRAWING':np.float32, 'DAYS_FIRST_DUE':np.float32, 'DAYS_LAST_DUE_1ST_VERSION':np.float32,\n",
    "        'DAYS_LAST_DUE':np.float32, 'DAYS_TERMINATION':np.float32, 'NFLAG_INSURED_ON_APPROVAL':np.float32\n",
    "    }\n",
    "    \n",
    "    bureau_dtype = {\n",
    "        'SK_ID_CURR':np.uint32, 'SK_ID_BUREAU':np.uint32, 'DAYS_CREDIT':np.int32,'CREDIT_DAY_OVERDUE':np.int32,\n",
    "        'CNT_CREDIT_PROLONG':np.int32, 'DAYS_CREDIT_UPDATE':np.int32, 'DAYS_CREDIT_ENDDATE':np.float32,\n",
    "        'DAYS_ENDDATE_FACT':np.float32, 'AMT_CREDIT_MAX_OVERDUE':np.float32, 'AMT_CREDIT_SUM':np.float32,\n",
    "        'AMT_CREDIT_SUM_DEBT':np.float32, 'AMT_CREDIT_SUM_LIMIT':np.float32, 'AMT_CREDIT_SUM_OVERDUE':np.float32,\n",
    "        'AMT_ANNUITY':np.float32\n",
    "    }\n",
    "    \n",
    "    bureau_bal_dtype = {\n",
    "        'SK_ID_BUREAU':np.int32, 'MONTHS_BALANCE':np.int32,\n",
    "    }\n",
    "    \n",
    "    pos_dtype = {\n",
    "        'SK_ID_PREV':np.uint32, 'SK_ID_CURR':np.uint32, 'MONTHS_BALANCE':np.int32, 'SK_DPD':np.int32,\n",
    "        'SK_DPD_DEF':np.int32, 'CNT_INSTALMENT':np.float32,'CNT_INSTALMENT_FUTURE':np.float32\n",
    "    }\n",
    "    \n",
    "    install_dtype = {\n",
    "        'SK_ID_PREV':np.uint32, 'SK_ID_CURR':np.uint32, 'NUM_INSTALMENT_NUMBER':np.int32, 'NUM_INSTALMENT_VERSION':np.float32,\n",
    "        'DAYS_INSTALMENT':np.float32, 'DAYS_ENTRY_PAYMENT':np.float32, 'AMT_INSTALMENT':np.float32, 'AMT_PAYMENT':np.float32\n",
    "    }\n",
    "    \n",
    "    card_dtype = {\n",
    "        'SK_ID_PREV':np.uint32, 'SK_ID_CURR':np.uint32, 'MONTHS_BALANCE':np.int16,\n",
    "        'AMT_CREDIT_LIMIT_ACTUAL':np.int32, 'CNT_DRAWINGS_CURRENT':np.int32, 'SK_DPD':np.int32,'SK_DPD_DEF':np.int32,\n",
    "        'AMT_BALANCE':np.float32, 'AMT_DRAWINGS_ATM_CURRENT':np.float32, 'AMT_DRAWINGS_CURRENT':np.float32,\n",
    "        'AMT_DRAWINGS_OTHER_CURRENT':np.float32, 'AMT_DRAWINGS_POS_CURRENT':np.float32, 'AMT_INST_MIN_REGULARITY':np.float32,\n",
    "        'AMT_PAYMENT_CURRENT':np.float32, 'AMT_PAYMENT_TOTAL_CURRENT':np.float32, 'AMT_RECEIVABLE_PRINCIPAL':np.float32,\n",
    "        'AMT_RECIVABLE':np.float32, 'AMT_TOTAL_RECEIVABLE':np.float32, 'CNT_DRAWINGS_ATM_CURRENT':np.float32,\n",
    "        'CNT_DRAWINGS_OTHER_CURRENT':np.float32, 'CNT_DRAWINGS_POS_CURRENT':np.float32, 'CNT_INSTALMENT_MATURE_CUM':np.float32\n",
    "    }\n",
    "    \n",
    "    app_train = pd.read_csv('dataset/application_train.csv')\n",
    "    app_test = pd.read_csv('dataset/application_test.csv')\n",
    "    apps = pd.concat([app_train, app_test])\n",
    "    prev = pd.read_csv('dataset/previous_application.csv', dtype=prev_dtype)\n",
    "    bureau = pd.read_csv('dataset/bureau.csv', dtype=bureau_dtype)\n",
    "    bureau_bal = pd.read_csv('dataset/bureau_balance.csv', dtype=bureau_bal_dtype)\n",
    "    pos_bal = pd.read_csv('dataset/POS_CASH_balance.csv', dtype=pos_dtype)\n",
    "    install = pd.read_csv('dataset/installments_payments.csv', dtype=install_dtype)\n",
    "    card_bal = pd.read_csv('dataset/credit_card_balance.csv', dtype=card_dtype)\n",
    "\n",
    "    return apps, prev, bureau, bureau_bal, pos_bal, install, card_bal\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5ocmIjSwCeWD"
   },
   "source": [
    "# Functions for feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2907,
     "status": "ok",
     "timestamp": 1599725320635,
     "user": {
      "displayName": "권철민",
      "photoUrl": "",
      "userId": "03917677622451543916"
     },
     "user_tz": -540
    },
    "id": "aaoj3PBECeWI"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "def get_apps_processed(apps):\n",
    "    \n",
    "    apps['APPS_EXT_SOURCE_MEAN'] = apps[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3']].mean(axis=1)\n",
    "    apps['APPS_EXT_SOURCE_STD'] = apps[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3']].std(axis=1)\n",
    "    apps['APPS_EXT_SOURCE_STD'] = apps['APPS_EXT_SOURCE_STD'].fillna(apps['APPS_EXT_SOURCE_STD'].mean())\n",
    "    \n",
    "    apps['APPS_ANNUITY_CREDIT_RATIO'] = apps['AMT_ANNUITY']/apps['AMT_CREDIT']\n",
    "    apps['APPS_GOODS_CREDIT_RATIO'] = apps['AMT_GOODS_PRICE']/apps['AMT_CREDIT']\n",
    "    \n",
    "    apps['APPS_ANNUITY_INCOME_RATIO'] = apps['AMT_ANNUITY']/apps['AMT_INCOME_TOTAL']\n",
    "    apps['APPS_CREDIT_INCOME_RATIO'] = apps['AMT_CREDIT']/apps['AMT_INCOME_TOTAL']\n",
    "    apps['APPS_GOODS_INCOME_RATIO'] = apps['AMT_GOODS_PRICE']/apps['AMT_INCOME_TOTAL']\n",
    "    apps['APPS_CNT_FAM_INCOME_RATIO'] = apps['AMT_INCOME_TOTAL']/apps['CNT_FAM_MEMBERS']\n",
    "    \n",
    "    apps['APPS_EMPLOYED_BIRTH_RATIO'] = apps['DAYS_EMPLOYED']/apps['DAYS_BIRTH']\n",
    "    apps['APPS_INCOME_EMPLOYED_RATIO'] = apps['AMT_INCOME_TOTAL']/apps['DAYS_EMPLOYED']\n",
    "    apps['APPS_INCOME_BIRTH_RATIO'] = apps['AMT_INCOME_TOTAL']/apps['DAYS_BIRTH']\n",
    "    apps['APPS_CAR_BIRTH_RATIO'] = apps['OWN_CAR_AGE'] / apps['DAYS_BIRTH']\n",
    "    apps['APPS_CAR_EMPLOYED_RATIO'] = apps['OWN_CAR_AGE'] / apps['DAYS_EMPLOYED']\n",
    "    \n",
    "    return apps\n",
    "\n",
    "def get_prev_processed(prev):\n",
    "    # The difference and ratio between the loan application amount and the actual loan amount/loan product amount\n",
    "    prev['PREV_CREDIT_DIFF'] = prev['AMT_APPLICATION'] - prev['AMT_CREDIT']\n",
    "    prev['PREV_GOODS_DIFF'] = prev['AMT_APPLICATION'] - prev['AMT_GOODS_PRICE']\n",
    "    prev['PREV_CREDIT_APPL_RATIO'] = prev['AMT_CREDIT']/prev['AMT_APPLICATION']\n",
    "    # prev['PREV_ANNUITY_APPL_RATIO'] = prev['AMT_ANNUITY']/prev['AMT_APPLICATION']\n",
    "    prev['PREV_GOODS_APPL_RATIO'] = prev['AMT_GOODS_PRICE']/prev['AMT_APPLICATION']\n",
    "    \n",
    "    prev['DAYS_FIRST_DRAWING'].replace(365243, np.nan, inplace= True)\n",
    "    prev['DAYS_FIRST_DUE'].replace(365243, np.nan, inplace= True)\n",
    "    prev['DAYS_LAST_DUE_1ST_VERSION'].replace(365243, np.nan, inplace= True)\n",
    "    prev['DAYS_LAST_DUE'].replace(365243, np.nan, inplace= True)\n",
    "    prev['DAYS_TERMINATION'].replace(365243, np.nan, inplace= True)\n",
    "    # the period from the first due date to the last due date\n",
    "    prev['PREV_DAYS_LAST_DUE_DIFF'] = prev['DAYS_LAST_DUE_1ST_VERSION'] - prev['DAYS_LAST_DUE']\n",
    "    # Calculate the total amount paid by multiplying the monthly amount by the number of payments.\n",
    "    all_pay = prev['AMT_ANNUITY'] * prev['CNT_PAYMENT']\n",
    "    # Calculate the interest rate by obtaining the AMT_CREDIT ratio to the total amount paid and dividing it by the number of payments again.\n",
    "    prev['PREV_INTERESTS_RATE'] = (all_pay/prev['AMT_CREDIT'] - 1)/prev['CNT_PAYMENT']\n",
    "        \n",
    "    return prev\n",
    "    \n",
    "    \n",
    "def get_prev_amt_agg(prev):\n",
    "    # Aggregation is carried out with a difference and ratio of different amounts compared to the newly created loan application amount.\n",
    "    agg_dict = {\n",
    "         # aggregation with the existing columns\n",
    "        'SK_ID_CURR':['count'],\n",
    "        'AMT_CREDIT':['mean', 'max', 'sum'],\n",
    "        'AMT_ANNUITY':['mean', 'max', 'sum'], \n",
    "        'AMT_APPLICATION':['mean', 'max', 'sum'],\n",
    "        'AMT_DOWN_PAYMENT':['mean', 'max', 'sum'],\n",
    "        'AMT_GOODS_PRICE':['mean', 'max', 'sum'],\n",
    "        'RATE_DOWN_PAYMENT': ['min', 'max', 'mean'],\n",
    "        'DAYS_DECISION': ['min', 'max', 'mean'],\n",
    "        'CNT_PAYMENT': ['mean', 'sum'],\n",
    "        # aggregation with the pre-processed columns\n",
    "        'PREV_CREDIT_DIFF':['mean', 'max', 'sum'], \n",
    "        'PREV_CREDIT_APPL_RATIO':['mean', 'max'],\n",
    "        'PREV_GOODS_DIFF':['mean', 'max', 'sum'],\n",
    "        'PREV_GOODS_APPL_RATIO':['mean', 'max'],\n",
    "        'PREV_DAYS_LAST_DUE_DIFF':['mean', 'max', 'sum'],\n",
    "        'PREV_INTERESTS_RATE':['mean', 'max']\n",
    "    }\n",
    "\n",
    "    prev_group = prev.groupby('SK_ID_CURR')\n",
    "    prev_amt_agg = prev_group.agg(agg_dict)\n",
    "\n",
    "    # change col names with underbar _\n",
    "    prev_amt_agg.columns = [\"PREV_\"+ \"_\".join(x).upper() for x in prev_amt_agg.columns.ravel()]\n",
    "    \n",
    "    return prev_amt_agg\n",
    "\n",
    "def get_prev_refused_appr_agg(prev):\n",
    "    # Perform groupby with the original groupby column + detailed reference column. Aggregates at a granular level and then transforms to an unstack() column level.\n",
    "    prev_refused_appr_group = prev[prev['NAME_CONTRACT_STATUS'].isin(['Approved', 'Refused'])].groupby([ 'SK_ID_CURR', 'NAME_CONTRACT_STATUS'])\n",
    "    prev_refused_appr_agg = prev_refused_appr_group['SK_ID_CURR'].count().unstack()\n",
    "    # change col names\n",
    "    prev_refused_appr_agg.columns = ['PREV_APPROVED_COUNT', 'PREV_REFUSED_COUNT' ]\n",
    "    # NaN into 0\n",
    "    prev_refused_appr_agg = prev_refused_appr_agg.fillna(0)\n",
    "    \n",
    "    return prev_refused_appr_agg\n",
    "\n",
    "#### DAYS_DECISION processed additional data sets prior to -365 days.\n",
    "def get_prev_days365_agg(prev):\n",
    "    cond_days365 = prev['DAYS_DECISION'] > -365\n",
    "    prev_days365_group = prev[cond_days365].groupby('SK_ID_CURR')\n",
    "    agg_dict = {\n",
    "         # aggregation with the existing columns\n",
    "        'SK_ID_CURR':['count'],\n",
    "        'AMT_CREDIT':['mean', 'max', 'sum'],\n",
    "        'AMT_ANNUITY':['mean', 'max', 'sum'], \n",
    "        'AMT_APPLICATION':['mean', 'max', 'sum'],\n",
    "        'AMT_DOWN_PAYMENT':['mean', 'max', 'sum'],\n",
    "        'AMT_GOODS_PRICE':['mean', 'max', 'sum'],\n",
    "        'RATE_DOWN_PAYMENT': ['min', 'max', 'mean'],\n",
    "        'DAYS_DECISION': ['min', 'max', 'mean'],\n",
    "        'CNT_PAYMENT': ['mean', 'sum'],\n",
    "        # aggregation with the pre-processed columns\n",
    "        'PREV_CREDIT_DIFF':['mean', 'max', 'sum'], \n",
    "        'PREV_CREDIT_APPL_RATIO':['mean', 'max'],\n",
    "        'PREV_GOODS_DIFF':['mean', 'max', 'sum'],\n",
    "        'PREV_GOODS_APPL_RATIO':['mean', 'max'],\n",
    "        'PREV_DAYS_LAST_DUE_DIFF':['mean', 'max', 'sum'],\n",
    "        'PREV_INTERESTS_RATE':['mean', 'max']\n",
    "    }\n",
    "    \n",
    "    prev_days365_agg = prev_days365_group.agg(agg_dict)\n",
    "\n",
    "    # change col names with underbar _\n",
    "    prev_days365_agg.columns = [\"PREV_D365_\"+ \"_\".join(x).upper() for x in prev_days365_agg.columns.ravel()]\n",
    "    \n",
    "    return prev_days365_agg\n",
    "    \n",
    "def get_prev_agg(prev):\n",
    "    prev = get_prev_processed(prev)\n",
    "    prev_amt_agg = get_prev_amt_agg(prev)\n",
    "    prev_refused_appr_agg = get_prev_refused_appr_agg(prev)\n",
    "    prev_days365_agg = get_prev_days365_agg(prev)\n",
    "    \n",
    "    # join with prev_amt_agg\n",
    "    prev_agg = prev_amt_agg.merge(prev_refused_appr_agg, on='SK_ID_CURR', how='left')\n",
    "    prev_agg = prev_agg.merge(prev_days365_agg, on='SK_ID_CURR', how='left')\n",
    "    # Generation of APPROVED_COUNT and REFUSED_COUNT ratio compared to past loans by SK_ID_CURR.\n",
    "    prev_agg['PREV_REFUSED_RATIO'] = prev_agg['PREV_REFUSED_COUNT']/prev_agg['PREV_SK_ID_CURR_COUNT']\n",
    "    prev_agg['PREV_APPROVED_RATIO'] = prev_agg['PREV_APPROVED_COUNT']/prev_agg['PREV_SK_ID_CURR_COUNT']\n",
    "    # Drop 'PREV_REFUSED_COUNT', 'PREV_APPROVED_COUNT'\n",
    "    prev_agg = prev_agg.drop(['PREV_REFUSED_COUNT', 'PREV_APPROVED_COUNT'], axis=1)\n",
    "    \n",
    "    return prev_agg\n",
    "\n",
    "# Processing of columns related to the debt amount relative to the date of completion of the debt and the loan amount.\n",
    "def get_bureau_processed(bureau):\n",
    "    # Processing the difference and date ratio between the scheduled debt start and finish date and the actual debt completion date.\n",
    "    bureau['BUREAU_ENDDATE_FACT_DIFF'] = bureau['DAYS_CREDIT_ENDDATE'] - bureau['DAYS_ENDDATE_FACT']\n",
    "    bureau['BUREAU_CREDIT_FACT_DIFF'] = bureau['DAYS_CREDIT'] - bureau['DAYS_ENDDATE_FACT']\n",
    "    bureau['BUREAU_CREDIT_ENDDATE_DIFF'] = bureau['DAYS_CREDIT'] - bureau['DAYS_CREDIT_ENDDATE']\n",
    "  \n",
    "    # Processing debt/loan ratio and difference\n",
    "    bureau['BUREAU_CREDIT_DEBT_RATIO']=bureau['AMT_CREDIT_SUM_DEBT']/bureau['AMT_CREDIT_SUM']\n",
    "    #bureau['BUREAU_CREDIT_DEBT_DIFF'] = bureau['AMT_CREDIT_SUM'] - bureau['AMT_CREDIT_SUM_DEBT']\n",
    "    bureau['BUREAU_CREDIT_DEBT_DIFF'] = bureau['AMT_CREDIT_SUM_DEBT'] - bureau['AMT_CREDIT_SUM']\n",
    "    \n",
    "    # Processing whether it is overdue or overdue for more than 120 days\n",
    "    bureau['BUREAU_IS_DPD'] = bureau['CREDIT_DAY_OVERDUE'].apply(lambda x: 1 if x > 0 else 0)\n",
    "    bureau['BUREAU_IS_DPD_OVER120'] = bureau['CREDIT_DAY_OVERDUE'].apply(lambda x: 1 if x >120 else 0)\n",
    "    \n",
    "    return bureau\n",
    "\n",
    "# Create an aggregation column at SK_ID_CURR level with major columns of Bureau and previously related columns of debt and loan amount.\n",
    "def get_bureau_day_amt_agg(bureau):\n",
    "        \n",
    "    bureau_agg_dict = {\n",
    "    'SK_ID_BUREAU':['count'],\n",
    "    'DAYS_CREDIT':['min', 'max', 'mean'],\n",
    "    'CREDIT_DAY_OVERDUE':['min', 'max', 'mean'],\n",
    "    'DAYS_CREDIT_ENDDATE':['min', 'max', 'mean'],\n",
    "    'DAYS_ENDDATE_FACT':['min', 'max', 'mean'],\n",
    "    'AMT_CREDIT_MAX_OVERDUE': ['max', 'mean'],\n",
    "    'AMT_CREDIT_SUM': ['max', 'mean', 'sum'],\n",
    "    'AMT_CREDIT_SUM_DEBT': ['max', 'mean', 'sum'],\n",
    "    'AMT_CREDIT_SUM_OVERDUE': ['max', 'mean', 'sum'],\n",
    "    'AMT_ANNUITY': ['max', 'mean', 'sum'],\n",
    "    # add pre-processed col\n",
    "    'BUREAU_ENDDATE_FACT_DIFF':['min', 'max', 'mean'],\n",
    "    'BUREAU_CREDIT_FACT_DIFF':['min', 'max', 'mean'],\n",
    "    'BUREAU_CREDIT_ENDDATE_DIFF':['min', 'max', 'mean'],\n",
    "    'BUREAU_CREDIT_DEBT_RATIO':['min', 'max', 'mean'],\n",
    "    'BUREAU_CREDIT_DEBT_DIFF':['min', 'max', 'mean'],\n",
    "    'BUREAU_IS_DPD':['mean', 'sum'],\n",
    "    'BUREAU_IS_DPD_OVER120':['mean', 'sum']\n",
    "    }\n",
    "\n",
    "    bureau_grp = bureau.groupby('SK_ID_CURR')\n",
    "    bureau_day_amt_agg = bureau_grp.agg(bureau_agg_dict)\n",
    "    bureau_day_amt_agg.columns = ['BUREAU_'+('_').join(column).upper() for column in bureau_day_amt_agg.columns.ravel()]\n",
    "    # Column SK_ID_CURR to reset_index() for join\n",
    "    bureau_day_amt_agg = bureau_day_amt_agg.reset_index()\n",
    "    #print('bureau_day_amt_agg shape:', bureau_day_amt_agg.shape)\n",
    "    return bureau_day_amt_agg\n",
    "\n",
    "# After filtering only the data that is CREDIT_ACTIVE='Active' in Bureau, create an aggregation column at the SK_ID_CURR level with the main column and the debt and loan amount related columns\n",
    "def get_bureau_active_agg(bureau):\n",
    "    # filtering CREDIT_ACTIVE='Active' \n",
    "    cond_active = bureau['CREDIT_ACTIVE'] == 'Active'\n",
    "    bureau_active_grp = bureau[cond_active].groupby(['SK_ID_CURR'])\n",
    "    bureau_agg_dict = {\n",
    "        'SK_ID_BUREAU':['count'],\n",
    "        'DAYS_CREDIT':['min', 'max', 'mean'],\n",
    "        'CREDIT_DAY_OVERDUE':['min', 'max', 'mean'],\n",
    "        'DAYS_CREDIT_ENDDATE':['min', 'max', 'mean'],\n",
    "        'DAYS_ENDDATE_FACT':['min', 'max', 'mean'],\n",
    "        'AMT_CREDIT_MAX_OVERDUE': ['max', 'mean'],\n",
    "        'AMT_CREDIT_SUM': ['max', 'mean', 'sum'],\n",
    "        'AMT_CREDIT_SUM_DEBT': ['max', 'mean', 'sum'],\n",
    "        'AMT_CREDIT_SUM_OVERDUE': ['max', 'mean', 'sum'],\n",
    "        'AMT_ANNUITY': ['max', 'mean', 'sum'],\n",
    "        # add pre-processed col\n",
    "        'BUREAU_ENDDATE_FACT_DIFF':['min', 'max', 'mean'],\n",
    "        'BUREAU_CREDIT_FACT_DIFF':['min', 'max', 'mean'],\n",
    "        'BUREAU_CREDIT_ENDDATE_DIFF':['min', 'max', 'mean'],\n",
    "        'BUREAU_CREDIT_DEBT_RATIO':['min', 'max', 'mean'],\n",
    "        'BUREAU_CREDIT_DEBT_DIFF':['min', 'max', 'mean'],\n",
    "        'BUREAU_IS_DPD':['mean', 'sum'],\n",
    "        'BUREAU_IS_DPD_OVER120':['mean', 'sum']\n",
    "        }\n",
    "\n",
    "    bureau_active_agg = bureau_active_grp.agg(bureau_agg_dict)\n",
    "    bureau_active_agg.columns = ['BUREAU_ACT_'+('_').join(column).upper() for column in bureau_active_agg.columns.ravel()]\n",
    "    # Column SK_ID_CURR for join with using reset_index()\n",
    "    bureau_active_agg = bureau_active_agg.reset_index()\n",
    "    #print('bureau_active_agg shape:', bureau_active_agg.shape)\n",
    "    return bureau_active_agg\n",
    "\n",
    "# BUREAU : DAYS_CREDIT > -750\n",
    "def get_bureau_days750_agg(bureau):\n",
    "    cond_days750 = bureau['DAYS_CREDIT'] > -750\n",
    "    bureau_days750_group = bureau[cond_days750].groupby('SK_ID_CURR')\n",
    "    bureau_agg_dict = {\n",
    "        'SK_ID_BUREAU':['count'],\n",
    "        'DAYS_CREDIT':['min', 'max', 'mean'],\n",
    "        'CREDIT_DAY_OVERDUE':['min', 'max', 'mean'],\n",
    "        'DAYS_CREDIT_ENDDATE':['min', 'max', 'mean'],\n",
    "        'DAYS_ENDDATE_FACT':['min', 'max', 'mean'],\n",
    "        'AMT_CREDIT_MAX_OVERDUE': ['max', 'mean'],\n",
    "        'AMT_CREDIT_SUM': ['max', 'mean', 'sum'],\n",
    "        'AMT_CREDIT_SUM_DEBT': ['max', 'mean', 'sum'],\n",
    "        'AMT_CREDIT_SUM_OVERDUE': ['max', 'mean', 'sum'],\n",
    "        'AMT_ANNUITY': ['max', 'mean', 'sum'],\n",
    "        # 추가 가공 컬럼\n",
    "        'BUREAU_ENDDATE_FACT_DIFF':['min', 'max', 'mean'],\n",
    "        'BUREAU_CREDIT_FACT_DIFF':['min', 'max', 'mean'],\n",
    "        'BUREAU_CREDIT_ENDDATE_DIFF':['min', 'max', 'mean'],\n",
    "        'BUREAU_CREDIT_DEBT_RATIO':['min', 'max', 'mean'],\n",
    "        'BUREAU_CREDIT_DEBT_DIFF':['min', 'max', 'mean'],\n",
    "        'BUREAU_IS_DPD':['mean', 'sum'],\n",
    "        'BUREAU_IS_DPD_OVER120':['mean', 'sum']\n",
    "        }\n",
    "\n",
    "    bureau_days750_agg = bureau_days750_group.agg(bureau_agg_dict)\n",
    "    bureau_days750_agg.columns = ['BUREAU_ACT_'+('_').join(column).upper() for column in bureau_days750_agg.columns.ravel()]\n",
    "    bureau_days750_agg = bureau_days750_agg.reset_index()\n",
    "    \n",
    "    return bureau_days750_agg\n",
    "\n",
    "\n",
    "# Processing the number of cases of brewau_bal to SK_ID_CURR level and the aggregation of MONTHS_BALANCE\n",
    "def get_bureau_bal_agg(bureau, bureau_bal):\n",
    "    # Performs a join to import the SK_ID_CURR column from the beureau to group by the SK_ID_CURR level.\n",
    "    bureau_bal = bureau_bal.merge(bureau[['SK_ID_CURR', 'SK_ID_BUREAU']], on='SK_ID_BUREAU', how='left')\n",
    "    \n",
    "    # Processing monthly overdue and 120 days overdue attributes according to STATUS.\n",
    "    bureau_bal['BUREAU_BAL_IS_DPD'] = bureau_bal['STATUS'].apply(lambda x: 1 if x in['1','2','3','4','5']  else 0)\n",
    "    bureau_bal['BUREAU_BAL_IS_DPD_OVER120'] = bureau_bal['STATUS'].apply(lambda x: 1 if x =='5'  else 0)\n",
    "    bureau_bal_grp = bureau_bal.groupby('SK_ID_CURR')\n",
    "    # Processing aggregation of number of cases and MONTHS_BALANCE at SK_ID_CURR level\n",
    "    bureau_bal_agg_dict = {\n",
    "        'SK_ID_CURR':['count'],\n",
    "        'MONTHS_BALANCE':['min', 'max', 'mean'],\n",
    "        'BUREAU_BAL_IS_DPD':['mean', 'sum'],\n",
    "        'BUREAU_BAL_IS_DPD_OVER120':['mean', 'sum']\n",
    "    }\n",
    "\n",
    "    bureau_bal_agg = bureau_bal_grp.agg(bureau_bal_agg_dict)\n",
    "    bureau_bal_agg.columns = [ 'BUREAU_BAL_'+('_').join(column).upper() for column in bureau_bal_agg.columns.ravel() ]\n",
    "    bureau_bal_agg = bureau_bal_agg.reset_index()\n",
    "    #print('bureau_bal_agg shape:', bureau_bal_agg.shape)\n",
    "    return bureau_bal_agg\n",
    "    \n",
    "# Combine all agg. columns related to bureau\n",
    "def get_bureau_agg(bureau, bureau_bal):\n",
    "    \n",
    "    bureau = get_bureau_processed(bureau)\n",
    "    bureau_day_amt_agg = get_bureau_day_amt_agg(bureau)\n",
    "    bureau_active_agg = get_bureau_active_agg(bureau)\n",
    "    bureau_days750_agg = get_bureau_days750_agg(bureau)\n",
    "    bureau_bal_agg = get_bureau_bal_agg(bureau, bureau_bal)\n",
    "    \n",
    "    # join bureau_day_amt_agg and bureau_active_agg \n",
    "    bureau_agg = bureau_day_amt_agg.merge(bureau_active_agg, on='SK_ID_CURR', how='left')\n",
    "    # STATUS가 ACTIVE IS_DPD RATIO관련 비율 재가공. \n",
    "    #bureau_agg['BUREAU_IS_DPD_RATIO'] = bureau_agg['BUREAU_BUREAU_IS_DPD_SUM']/bureau_agg['BUREAU_SK_ID_BUREAU_COUNT']\n",
    "    #bureau_agg['BUREAU_IS_DPD_OVER120_RATIO'] = bureau_agg['BUREAU_BUREAU_IS_DPD_OVER120_SUM']/bureau_agg['BUREAU_SK_ID_BUREAU_COUNT']\n",
    "    bureau_agg['BUREAU_ACT_IS_DPD_RATIO'] = bureau_agg['BUREAU_ACT_BUREAU_IS_DPD_SUM']/bureau_agg['BUREAU_SK_ID_BUREAU_COUNT']\n",
    "    bureau_agg['BUREAU_ACT_IS_DPD_OVER120_RATIO'] = bureau_agg['BUREAU_ACT_BUREAU_IS_DPD_OVER120_SUM']/bureau_agg['BUREAU_SK_ID_BUREAU_COUNT']\n",
    "    \n",
    "    # join \n",
    "    bureau_agg = bureau_agg.merge(bureau_bal_agg, on='SK_ID_CURR', how='left')\n",
    "    bureau_agg = bureau_agg.merge(bureau_days750_agg, on='SK_ID_CURR', how='left') \n",
    "    #bureau_bal_agg['BUREAU_BAL_IS_DPD_RATIO'] = bureau_bal_agg['BUREAU_BAL_BUREAU_BAL_IS_DPD_SUM']/bureau_bal_agg['BUREAU_BAL_SK_ID_CURR_COUNT']\n",
    "    #bureau_bal_agg['BUREAU_BAL_IS_DPD_OVER120_RATIO'] = bureau_bal_agg['BUREAU_BAL_BUREAU_BAL_IS_DPD_OVER120_SUM']/bureau_bal_agg['BUREAU_BAL_SK_ID_CURR_COUNT']\n",
    "\n",
    "    #print('bureau_agg shape:', bureau_agg.shape)\n",
    "    \n",
    "    return bureau_agg\n",
    "\n",
    "def get_pos_bal_agg(pos_bal):\n",
    "  \n",
    "    # Whether it is overdue\n",
    "    pos_bal['POS_IS_DPD'] = pos_bal['SK_DPD'].apply(lambda x: 1 if x > 0 else 0)\n",
    "    # whether the number of overdue days is between 0 and 120, and whether the number of overdue days is greater than 120\n",
    "    pos_bal['POS_IS_DPD_UNDER_120'] = pos_bal['SK_DPD'].apply(lambda x:1 if (x > 0) & (x <120) else 0 )\n",
    "    pos_bal['POS_IS_DPD_OVER_120'] = pos_bal['SK_DPD'].apply(lambda x:1 if x >= 120 else 0)\n",
    "\n",
    "    # Create new aggregation column with SK_ID_CURR level with existing and new columns\n",
    "    pos_bal_grp = pos_bal.groupby('SK_ID_CURR')\n",
    "    pos_bal_agg_dict = {\n",
    "        'SK_ID_CURR':['count'], \n",
    "        'MONTHS_BALANCE':['min', 'mean', 'max'], \n",
    "        'SK_DPD':['min', 'max', 'mean', 'sum'],\n",
    "        'CNT_INSTALMENT':['min', 'max', 'mean', 'sum'],\n",
    "        'CNT_INSTALMENT_FUTURE':['min', 'max', 'mean', 'sum'],\n",
    "        # Added\n",
    "        'POS_IS_DPD':['mean', 'sum'],\n",
    "        'POS_IS_DPD_UNDER_120':['mean', 'sum'],\n",
    "        'POS_IS_DPD_OVER_120':['mean', 'sum']\n",
    "    }\n",
    "\n",
    "    pos_bal_agg = pos_bal_grp.agg(pos_bal_agg_dict)\n",
    "    # Change col name\n",
    "    pos_bal_agg.columns = [('POS_')+('_').join(column).upper() for column in pos_bal_agg.columns.ravel()]\n",
    "    \n",
    "    # Add new datasets with MONTHS_BALANCE last (20 months or less)\n",
    "    cond_months = pos_bal['MONTHS_BALANCE'] > -20\n",
    "    pos_bal_m20_grp = pos_bal[cond_months].groupby('SK_ID_CURR')\n",
    "    pos_bal_m20_agg_dict = {\n",
    "        'SK_ID_CURR':['count'], \n",
    "        'MONTHS_BALANCE':['min', 'mean', 'max'], \n",
    "        'SK_DPD':['min', 'max', 'mean', 'sum'],\n",
    "        'CNT_INSTALMENT':['min', 'max', 'mean', 'sum'],\n",
    "        'CNT_INSTALMENT_FUTURE':['min', 'max', 'mean', 'sum'],\n",
    "        # Added\n",
    "        'POS_IS_DPD':['mean', 'sum'],\n",
    "        'POS_IS_DPD_UNDER_120':['mean', 'sum'],\n",
    "        'POS_IS_DPD_OVER_120':['mean', 'sum']\n",
    "    }\n",
    "\n",
    "    pos_bal_m20_agg = pos_bal_m20_grp.agg(pos_bal_m20_agg_dict)\n",
    "    # Change column\n",
    "    pos_bal_m20_agg.columns = [('POS_M20')+('_').join(column).upper() for column in pos_bal_m20_agg.columns.ravel()]\n",
    "    pos_bal_agg = pos_bal_agg.merge(pos_bal_m20_agg, on='SK_ID_CURR', how='left')\n",
    "    \n",
    "    # Change SK_ID_CURR with reset_index()\n",
    "    pos_bal_agg = pos_bal_agg.reset_index()\n",
    "    \n",
    "    \n",
    "    return pos_bal_agg\n",
    "\n",
    "def get_install_agg(install):\n",
    "    # Processing data related to the actual payment amount compared to the scheduled payment amount. Create a DPD date by comparing the actual payment date to the scheduled payment date\n",
    "    install['AMT_DIFF'] = install['AMT_INSTALMENT'] - install['AMT_PAYMENT']\n",
    "    install['AMT_RATIO'] =  (install['AMT_PAYMENT'] +1)/ (install['AMT_INSTALMENT'] + 1)\n",
    "    install['SK_DPD'] = install['DAYS_ENTRY_PAYMENT'] - install['DAYS_INSTALMENT']\n",
    "\n",
    "    # Processing data on whether it is overdue, whether the number of overdue days is between 30 and 120, and whether the number of overdue days is greater than 100.\n",
    "    install['INS_IS_DPD'] = install['SK_DPD'].apply(lambda x: 1 if x > 0 else 0)\n",
    "    install['INS_IS_DPD_UNDER_120'] = install['SK_DPD'].apply(lambda x:1 if (x > 0) & (x <120) else 0 )\n",
    "    install['INS_IS_DPD_OVER_120'] = install['SK_DPD'].apply(lambda x:1 if x >= 120 else 0)\n",
    "\n",
    "    # Create new aggregation column with SK_ID_CURR level with existing and new columns.\n",
    "    install_grp = install.groupby('SK_ID_CURR')\n",
    "\n",
    "    install_agg_dict = {\n",
    "        'SK_ID_CURR':['count'],\n",
    "        'NUM_INSTALMENT_VERSION':['nunique'], \n",
    "        'DAYS_ENTRY_PAYMENT':['mean', 'max', 'sum'],\n",
    "        'DAYS_INSTALMENT':['mean', 'max', 'sum'],\n",
    "        'AMT_INSTALMENT':['mean', 'max', 'sum'],\n",
    "        'AMT_PAYMENT':['mean', 'max','sum'],\n",
    "        # added\n",
    "        'AMT_DIFF':['mean','min', 'max','sum'],\n",
    "        'AMT_RATIO':['mean', 'max'],\n",
    "        'SK_DPD':['mean', 'min', 'max'],\n",
    "        'INS_IS_DPD':['mean', 'sum'],\n",
    "        'INS_IS_DPD_UNDER_120':['mean', 'sum'],\n",
    "        'INS_IS_DPD_OVER_120':['mean', 'sum']    \n",
    "    }\n",
    "\n",
    "    install_agg = install_grp.agg(install_agg_dict)\n",
    "    install_agg.columns = ['INS_'+('_').join(column).upper() for column in install_agg.columns.ravel()]\n",
    "\n",
    "    \n",
    "    # Actual payment date (DAYS_ENTRY_PAYMENT) is relatively recent (within 1 year) only processed separately\n",
    "    cond_day = install['DAYS_ENTRY_PAYMENT'] >= -365\n",
    "    install_d365_grp = install[cond_day].groupby('SK_ID_CURR')\n",
    "    install_d365_agg_dict = {\n",
    "        'SK_ID_CURR':['count'],\n",
    "        'NUM_INSTALMENT_VERSION':['nunique'], \n",
    "        'DAYS_ENTRY_PAYMENT':['mean', 'max', 'sum'],\n",
    "        'DAYS_INSTALMENT':['mean', 'max', 'sum'],\n",
    "        'AMT_INSTALMENT':['mean', 'max', 'sum'],\n",
    "        'AMT_PAYMENT':['mean', 'max','sum'],\n",
    "        # added\n",
    "        'AMT_DIFF':['mean','min', 'max','sum'],\n",
    "        'AMT_RATIO':['mean', 'max'],\n",
    "        'SK_DPD':['mean', 'min', 'max'],\n",
    "        'INS_IS_DPD':['mean', 'sum'],\n",
    "        'INS_IS_DPD_UNDER_120':['mean', 'sum'],\n",
    "        'INS_IS_DPD_OVER_120':['mean', 'sum']    \n",
    "    }\n",
    "    \n",
    "    install_d365_agg = install_d365_grp.agg(install_d365_agg_dict)\n",
    "    install_d365_agg.columns = ['INS_D365'+('_').join(column).upper() for column in install_d365_agg.columns.ravel()]\n",
    "    \n",
    "    install_agg = install_agg.merge(install_d365_agg, on='SK_ID_CURR', how='left')\n",
    "    install_agg = install_agg.reset_index()\n",
    "    \n",
    "    return install_agg\n",
    "\n",
    "def get_card_bal_agg(card_bal):\n",
    "    # Percentage of balance and withdrawal amount based on monthly card allowance\n",
    "    card_bal['BALANCE_LIMIT_RATIO'] = card_bal['AMT_BALANCE']/card_bal['AMT_CREDIT_LIMIT_ACTUAL']\n",
    "    card_bal['DRAWING_LIMIT_RATIO'] = card_bal['AMT_DRAWINGS_CURRENT'] / card_bal['AMT_CREDIT_LIMIT_ACTUAL']\n",
    "\n",
    "    # Processing column generation according to DPD.\n",
    "    card_bal['CARD_IS_DPD'] = card_bal['SK_DPD'].apply(lambda x: 1 if x > 0 else 0)\n",
    "    card_bal['CARD_IS_DPD_UNDER_120'] = card_bal['SK_DPD'].apply(lambda x:1 if (x > 0) & (x <120) else 0 )\n",
    "    card_bal['CARD_IS_DPD_OVER_120'] = card_bal['SK_DPD'].apply(lambda x:1 if x >= 120 else 0)\n",
    "\n",
    "    # Create new aggregation columns with existing and machining columns at SK_ID_CURR level.\n",
    "    card_bal_grp = card_bal.groupby('SK_ID_CURR')\n",
    "    card_bal_agg_dict = {\n",
    "        'SK_ID_CURR':['count'],\n",
    "         #'MONTHS_BALANCE':['min', 'max', 'mean'],\n",
    "        'AMT_BALANCE':['max'],\n",
    "        'AMT_CREDIT_LIMIT_ACTUAL':['max'],\n",
    "        'AMT_DRAWINGS_ATM_CURRENT': ['max', 'sum'],\n",
    "        'AMT_DRAWINGS_CURRENT': ['max', 'sum'],\n",
    "        'AMT_DRAWINGS_POS_CURRENT': ['max', 'sum'],\n",
    "        'AMT_INST_MIN_REGULARITY': ['max', 'mean'],\n",
    "        'AMT_PAYMENT_TOTAL_CURRENT': ['max','sum'],\n",
    "        'AMT_TOTAL_RECEIVABLE': ['max', 'mean'],\n",
    "        'CNT_DRAWINGS_ATM_CURRENT': ['max','sum'],\n",
    "        'CNT_DRAWINGS_CURRENT': ['max', 'mean', 'sum'],\n",
    "        'CNT_DRAWINGS_POS_CURRENT': ['mean'],\n",
    "        'SK_DPD': ['mean', 'max', 'sum'],\n",
    "        # added\n",
    "        'BALANCE_LIMIT_RATIO':['min','max'],\n",
    "        'DRAWING_LIMIT_RATIO':['min', 'max'],\n",
    "        'CARD_IS_DPD':['mean', 'sum'],\n",
    "        'CARD_IS_DPD_UNDER_120':['mean', 'sum'],\n",
    "        'CARD_IS_DPD_OVER_120':['mean', 'sum']    \n",
    "    }\n",
    "    card_bal_agg = card_bal_grp.agg(card_bal_agg_dict)\n",
    "    card_bal_agg.columns = ['CARD_'+('_').join(column).upper() for column in card_bal_agg.columns.ravel()]\n",
    "\n",
    "    card_bal_agg = card_bal_agg.reset_index()\n",
    "    \n",
    "    # MONTHS_BALANCE processed relatively recent data (less than 3 months) separately.\n",
    "    cond_month = card_bal.MONTHS_BALANCE >= -3\n",
    "    card_bal_m3_grp = card_bal[cond_month].groupby('SK_ID_CURR')\n",
    "    card_bal_m3_agg = card_bal_m3_grp.agg(card_bal_agg_dict)\n",
    "    card_bal_m3_agg.columns = ['CARD_M3'+('_').join(column).upper() for column in card_bal_m3_agg.columns.ravel()]\n",
    "    \n",
    "    card_bal_agg = card_bal_agg.merge(card_bal_m3_agg, on='SK_ID_CURR', how='left')\n",
    "    card_bal_agg = card_bal_agg.reset_index()\n",
    "    \n",
    "    return card_bal_agg\n",
    "\n",
    "def get_apps_all_encoded(apps_all):\n",
    "    object_columns = apps_all.dtypes[apps_all.dtypes == 'object'].index.tolist()\n",
    "    for column in object_columns:\n",
    "        apps_all[column] = pd.factorize(apps_all[column])[0]\n",
    "    \n",
    "    return apps_all\n",
    "\n",
    "def get_apps_all_train_test(apps_all):\n",
    "    apps_all_train = apps_all[~apps_all['TARGET'].isnull()]\n",
    "    apps_all_test = apps_all[apps_all['TARGET'].isnull()]\n",
    "\n",
    "    apps_all_test = apps_all_test.drop('TARGET', axis=1)\n",
    "    \n",
    "    return apps_all_train, apps_all_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2097,
     "status": "ok",
     "timestamp": 1599725320638,
     "user": {
      "displayName": "권철민",
      "photoUrl": "",
      "userId": "03917677622451543916"
     },
     "user_tz": -540
    },
    "id": "uifgJbUWCeWR"
   },
   "outputs": [],
   "source": [
    "# Create and join apps and prev_agg, brew_agg, pos_bal_agg, install_agg, card_bal_agg by calling individual functions\n",
    "def get_apps_all_with_all_agg(apps, prev, bureau, bureau_bal, pos_bal, install, card_bal):\n",
    "    apps_all =  get_apps_processed(apps)\n",
    "    prev_agg = get_prev_agg(prev)\n",
    "    bureau_agg = get_bureau_agg(bureau, bureau_bal)\n",
    "    pos_bal_agg = get_pos_bal_agg(pos_bal)\n",
    "    install_agg = get_install_agg(install)\n",
    "    card_bal_agg = get_card_bal_agg(card_bal)\n",
    "    print('prev_agg shape:', prev_agg.shape, 'bureau_agg shape:', bureau_agg.shape )\n",
    "    print('pos_bal_agg shape:', pos_bal_agg.shape, 'install_agg shape:', install_agg.shape, 'card_bal_agg shape:', card_bal_agg.shape)\n",
    "    print('apps_all before merge shape:', apps_all.shape)\n",
    "    \n",
    "    # Join the generated prev_agg, brewau_agg, pos_bal_agg, install_agg, card_bal_agg with apps to generate the final learning/test set.\n",
    "    apps_all = apps_all.merge(prev_agg, on='SK_ID_CURR', how='left')\n",
    "    apps_all = apps_all.merge(bureau_agg, on='SK_ID_CURR', how='left')\n",
    "    apps_all = apps_all.merge(pos_bal_agg, on='SK_ID_CURR', how='left')\n",
    "    apps_all = apps_all.merge(install_agg, on='SK_ID_CURR', how='left')\n",
    "    apps_all = apps_all.merge(card_bal_agg, on='SK_ID_CURR', how='left')\n",
    "    \n",
    "    print('apps_all after merge with all shape:', apps_all.shape)\n",
    "    \n",
    "    return apps_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lDD1hDA0CeWe"
   },
   "source": [
    "# Get data for training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 92
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 161849,
     "status": "ok",
     "timestamp": 1599725481976,
     "user": {
      "displayName": "권철민",
      "photoUrl": "",
      "userId": "03917677622451543916"
     },
     "user_tz": -540
    },
    "id": "rO2ml4oCCeWf",
    "outputId": "da51b95d-2849-42d8-f5ef-60a30070b9c3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/df/19n86p1s63b0654c8m28bsg80000gn/T/ipykernel_68061/553919847.py:79: FutureWarning: Index.ravel returning ndarray is deprecated; in a future version this will return a view on self.\n",
      "  prev_amt_agg.columns = [\"PREV_\"+ \"_\".join(x).upper() for x in prev_amt_agg.columns.ravel()]\n",
      "/var/folders/df/19n86p1s63b0654c8m28bsg80000gn/T/ipykernel_68061/553919847.py:121: FutureWarning: Index.ravel returning ndarray is deprecated; in a future version this will return a view on self.\n",
      "  prev_days365_agg.columns = [\"PREV_D365_\"+ \"_\".join(x).upper() for x in prev_days365_agg.columns.ravel()]\n",
      "/var/folders/df/19n86p1s63b0654c8m28bsg80000gn/T/ipykernel_68061/553919847.py:186: FutureWarning: Index.ravel returning ndarray is deprecated; in a future version this will return a view on self.\n",
      "  bureau_day_amt_agg.columns = ['BUREAU_'+('_').join(column).upper() for column in bureau_day_amt_agg.columns.ravel()]\n",
      "/var/folders/df/19n86p1s63b0654c8m28bsg80000gn/T/ipykernel_68061/553919847.py:219: FutureWarning: Index.ravel returning ndarray is deprecated; in a future version this will return a view on self.\n",
      "  bureau_active_agg.columns = ['BUREAU_ACT_'+('_').join(column).upper() for column in bureau_active_agg.columns.ravel()]\n",
      "/var/folders/df/19n86p1s63b0654c8m28bsg80000gn/T/ipykernel_68061/553919847.py:251: FutureWarning: Index.ravel returning ndarray is deprecated; in a future version this will return a view on self.\n",
      "  bureau_days750_agg.columns = ['BUREAU_ACT_'+('_').join(column).upper() for column in bureau_days750_agg.columns.ravel()]\n",
      "/var/folders/df/19n86p1s63b0654c8m28bsg80000gn/T/ipykernel_68061/553919847.py:275: FutureWarning: Index.ravel returning ndarray is deprecated; in a future version this will return a view on self.\n",
      "  bureau_bal_agg.columns = [ 'BUREAU_BAL_'+('_').join(column).upper() for column in bureau_bal_agg.columns.ravel() ]\n",
      "/var/folders/df/19n86p1s63b0654c8m28bsg80000gn/T/ipykernel_68061/553919847.py:331: FutureWarning: Index.ravel returning ndarray is deprecated; in a future version this will return a view on self.\n",
      "  pos_bal_agg.columns = [('POS_')+('_').join(column).upper() for column in pos_bal_agg.columns.ravel()]\n",
      "/var/folders/df/19n86p1s63b0654c8m28bsg80000gn/T/ipykernel_68061/553919847.py:350: FutureWarning: Index.ravel returning ndarray is deprecated; in a future version this will return a view on self.\n",
      "  pos_bal_m20_agg.columns = [('POS_M20')+('_').join(column).upper() for column in pos_bal_m20_agg.columns.ravel()]\n",
      "/var/folders/df/19n86p1s63b0654c8m28bsg80000gn/T/ipykernel_68061/553919847.py:390: FutureWarning: Index.ravel returning ndarray is deprecated; in a future version this will return a view on self.\n",
      "  install_agg.columns = ['INS_'+('_').join(column).upper() for column in install_agg.columns.ravel()]\n",
      "/var/folders/df/19n86p1s63b0654c8m28bsg80000gn/T/ipykernel_68061/553919847.py:413: FutureWarning: Index.ravel returning ndarray is deprecated; in a future version this will return a view on self.\n",
      "  install_d365_agg.columns = ['INS_D365'+('_').join(column).upper() for column in install_d365_agg.columns.ravel()]\n",
      "/var/folders/df/19n86p1s63b0654c8m28bsg80000gn/T/ipykernel_68061/553919847.py:455: FutureWarning: Index.ravel returning ndarray is deprecated; in a future version this will return a view on self.\n",
      "  card_bal_agg.columns = ['CARD_'+('_').join(column).upper() for column in card_bal_agg.columns.ravel()]\n",
      "/var/folders/df/19n86p1s63b0654c8m28bsg80000gn/T/ipykernel_68061/553919847.py:463: FutureWarning: Index.ravel returning ndarray is deprecated; in a future version this will return a view on self.\n",
      "  card_bal_m3_agg.columns = ['CARD_M3'+('_').join(column).upper() for column in card_bal_m3_agg.columns.ravel()]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prev_agg shape: (338857, 80) bureau_agg shape: (305811, 149)\n",
      "pos_bal_agg shape: (337252, 45) install_agg shape: (339587, 59) card_bal_agg shape: (103558, 70)\n",
      "apps_all before merge shape: (356255, 135)\n",
      "apps_all after merge with all shape: (356255, 534)\n"
     ]
    }
   ],
   "source": [
    "apps, prev, bureau, bureau_bal, pos_bal, install, card_bal = get_dataset()\n",
    "\n",
    "# application, previous, bureau, bureau_bal \n",
    "apps_all = get_apps_all_with_all_agg(apps, prev, bureau, bureau_bal, pos_bal, install, card_bal)\n",
    "\n",
    "# Label encoding with Category columns\n",
    "apps_all = get_apps_all_encoded(apps_all)\n",
    "\n",
    "# Split training and testing\n",
    "apps_all_train, apps_all_test = get_apps_all_train_test(apps_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kQm5zVNaCeWt"
   },
   "source": [
    "# Ouf Of Fold Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 157565,
     "status": "ok",
     "timestamp": 1599725481981,
     "user": {
      "displayName": "권철민",
      "photoUrl": "",
      "userId": "03917677622451543916"
     },
     "user_tz": -540
    },
    "id": "RDAo4NwLCeWv"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "def train_apps_all_with_oof(apps_all_train, apps_all_test, nfolds=5):\n",
    "    ftr_app = apps_all_train.drop(['SK_ID_CURR', 'TARGET'], axis=1)\n",
    "    target_app = apps_all_train['TARGET']\n",
    "\n",
    "    # Change double-type hyperparameters to int \n",
    "    folds = KFold(n_splits=nfolds, shuffle=True, random_state=2020)\n",
    "    \n",
    "    # Create an array to contain the resulting probabilities by predicting the validation set of models trained with OOF\n",
    "    # The size must be the size of ftr_app because the validation set is as many as n_split.\n",
    "    oof_preds = np.zeros(ftr_app.shape[0])  \n",
    "    \n",
    "    # Create an array to contain the result probability by predicting the test dataset of the model learned with Ou of Folds.\n",
    "    test_preds = np.zeros(apps_all_test.shape[0])\n",
    "    \n",
    "    # n_estimators 4000\n",
    "    clf = LGBMClassifier(\n",
    "                nthread=4,\n",
    "                n_estimators=4000,\n",
    "                learning_rate=0.01,\n",
    "                max_depth = 11,\n",
    "                num_leaves=58,\n",
    "                colsample_bytree=0.613,\n",
    "                subsample=0.708,\n",
    "                max_bin=407,\n",
    "                reg_alpha=3.564,\n",
    "                reg_lambda=4.930,\n",
    "                min_child_weight= 6,\n",
    "                min_child_samples=165,\n",
    "                silent=-1,\n",
    "                verbose=-1,\n",
    "                )\n",
    "\n",
    "    # Nfolds number cross validation Iteration Repeated and OOF method of predicting learning and test data\n",
    "    for fold_idx, (train_idx, valid_idx) in enumerate(folds.split(ftr_app)):\n",
    "        print('##### iteration ', fold_idx, ' 시작')\n",
    "        # Extract learning/verification data based on the index of the learning data set and the index of the verification data set\n",
    "        train_x  = ftr_app.iloc[train_idx, :]\n",
    "        train_y = target_app.iloc[train_idx]\n",
    "        valid_x = ftr_app.iloc[valid_idx, :]\n",
    "        valid_y = target_app.iloc[valid_idx]\n",
    "        \n",
    "        # Model learning with extracted learning/validation datasets. Early_stoping is increased to 200.\n",
    "        clf.fit(train_x, train_y, eval_set=[(train_x, train_y), (valid_x, valid_y)], eval_metric= 'auc', verbose= 200, \n",
    "                early_stopping_rounds= 200)\n",
    "        # Store probabilities predicted by the validation data set. Not used.\n",
    "        oof_preds[valid_idx] = clf.predict_proba(valid_x, num_iteration=clf.best_iteration_)[:, 1]       \n",
    "        # Compute prediction probabilities on test datasets with learned models.\n",
    "        # Calculate the final average probability by adding the probability divided by the number of times each individual performance to obtain the average probability because nfolds are repeated.\n",
    "        test_preds += clf.predict_proba(apps_all_test.drop('SK_ID_CURR', axis=1), num_iteration=clf.best_iteration_)[:, 1]/folds.n_splits\n",
    "        \n",
    "        \n",
    "    return clf, test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 205
    },
    "colab_type": "code",
    "id": "bAMSJm3gCeW6",
    "outputId": "5b1fa266-e728-415a-b3bb-772c8f647f89"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-08-29 16:22:29.657482\n",
      "##### iteration  0  시작\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chloe/opt/anaconda3/lib/python3.9/site-packages/lightgbm/sklearn.py:598: UserWarning: 'silent' argument is deprecated and will be removed in a future release of LightGBM. Pass 'verbose' parameter via keyword arguments instead.\n",
      "  _log_warning(\"'silent' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "/Users/chloe/opt/anaconda3/lib/python3.9/site-packages/lightgbm/sklearn.py:726: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "/Users/chloe/opt/anaconda3/lib/python3.9/site-packages/lightgbm/sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[200]\ttraining's auc: 0.795266\ttraining's binary_logloss: 0.239608\tvalid_1's auc: 0.772176\tvalid_1's binary_logloss: 0.242323\n",
      "[400]\ttraining's auc: 0.81807\ttraining's binary_logloss: 0.228423\tvalid_1's auc: 0.783351\tvalid_1's binary_logloss: 0.236437\n",
      "[600]\ttraining's auc: 0.833897\ttraining's binary_logloss: 0.221226\tvalid_1's auc: 0.788567\tvalid_1's binary_logloss: 0.234112\n",
      "[800]\ttraining's auc: 0.847203\ttraining's binary_logloss: 0.215417\tvalid_1's auc: 0.791229\tvalid_1's binary_logloss: 0.232989\n",
      "[1000]\ttraining's auc: 0.858902\ttraining's binary_logloss: 0.210354\tvalid_1's auc: 0.792717\tvalid_1's binary_logloss: 0.232354\n",
      "[1200]\ttraining's auc: 0.869301\ttraining's binary_logloss: 0.205808\tvalid_1's auc: 0.793834\tvalid_1's binary_logloss: 0.231905\n",
      "[1400]\ttraining's auc: 0.878656\ttraining's binary_logloss: 0.201618\tvalid_1's auc: 0.794389\tvalid_1's binary_logloss: 0.231658\n",
      "[1600]\ttraining's auc: 0.887134\ttraining's binary_logloss: 0.197708\tvalid_1's auc: 0.794656\tvalid_1's binary_logloss: 0.231517\n",
      "[1800]\ttraining's auc: 0.894927\ttraining's binary_logloss: 0.194008\tvalid_1's auc: 0.794772\tvalid_1's binary_logloss: 0.231433\n",
      "[2000]\ttraining's auc: 0.901993\ttraining's binary_logloss: 0.190515\tvalid_1's auc: 0.795016\tvalid_1's binary_logloss: 0.231346\n",
      "[2200]\ttraining's auc: 0.908609\ttraining's binary_logloss: 0.187109\tvalid_1's auc: 0.795083\tvalid_1's binary_logloss: 0.231319\n",
      "[2400]\ttraining's auc: 0.914687\ttraining's binary_logloss: 0.183883\tvalid_1's auc: 0.795163\tvalid_1's binary_logloss: 0.231292\n",
      "[2600]\ttraining's auc: 0.920518\ttraining's binary_logloss: 0.180721\tvalid_1's auc: 0.795209\tvalid_1's binary_logloss: 0.231266\n",
      "[2800]\ttraining's auc: 0.92596\ttraining's binary_logloss: 0.177621\tvalid_1's auc: 0.795338\tvalid_1's binary_logloss: 0.231233\n",
      "[3000]\ttraining's auc: 0.930888\ttraining's binary_logloss: 0.174689\tvalid_1's auc: 0.795355\tvalid_1's binary_logloss: 0.231233\n",
      "##### iteration  1  시작\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chloe/opt/anaconda3/lib/python3.9/site-packages/lightgbm/sklearn.py:598: UserWarning: 'silent' argument is deprecated and will be removed in a future release of LightGBM. Pass 'verbose' parameter via keyword arguments instead.\n",
      "  _log_warning(\"'silent' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "/Users/chloe/opt/anaconda3/lib/python3.9/site-packages/lightgbm/sklearn.py:726: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "/Users/chloe/opt/anaconda3/lib/python3.9/site-packages/lightgbm/sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] num_threads is set with nthread=4, will be overridden by n_jobs=-1. Current value: num_threads=-1\n",
      "[200]\ttraining's auc: 0.795736\ttraining's binary_logloss: 0.238197\tvalid_1's auc: 0.769512\tvalid_1's binary_logloss: 0.247811\n",
      "[400]\ttraining's auc: 0.818363\ttraining's binary_logloss: 0.227036\tvalid_1's auc: 0.78216\tvalid_1's binary_logloss: 0.241662\n",
      "[600]\ttraining's auc: 0.834182\ttraining's binary_logloss: 0.219849\tvalid_1's auc: 0.788158\tvalid_1's binary_logloss: 0.239205\n",
      "[800]\ttraining's auc: 0.847315\ttraining's binary_logloss: 0.214116\tvalid_1's auc: 0.791324\tvalid_1's binary_logloss: 0.237997\n",
      "[1000]\ttraining's auc: 0.858859\ttraining's binary_logloss: 0.209077\tvalid_1's auc: 0.793213\tvalid_1's binary_logloss: 0.237286\n",
      "[1200]\ttraining's auc: 0.868972\ttraining's binary_logloss: 0.204576\tvalid_1's auc: 0.79438\tvalid_1's binary_logloss: 0.23681\n",
      "[1400]\ttraining's auc: 0.878086\ttraining's binary_logloss: 0.200405\tvalid_1's auc: 0.795042\tvalid_1's binary_logloss: 0.236549\n",
      "[1600]\ttraining's auc: 0.886494\ttraining's binary_logloss: 0.196488\tvalid_1's auc: 0.795452\tvalid_1's binary_logloss: 0.236392\n",
      "[1800]\ttraining's auc: 0.894151\ttraining's binary_logloss: 0.192839\tvalid_1's auc: 0.795661\tvalid_1's binary_logloss: 0.236313\n",
      "[2000]\ttraining's auc: 0.901334\ttraining's binary_logloss: 0.189349\tvalid_1's auc: 0.795854\tvalid_1's binary_logloss: 0.236247\n",
      "[2200]\ttraining's auc: 0.90794\ttraining's binary_logloss: 0.185964\tvalid_1's auc: 0.795903\tvalid_1's binary_logloss: 0.236231\n",
      "[2400]\ttraining's auc: 0.914142\ttraining's binary_logloss: 0.182716\tvalid_1's auc: 0.795989\tvalid_1's binary_logloss: 0.236206\n",
      "[2600]\ttraining's auc: 0.920115\ttraining's binary_logloss: 0.179522\tvalid_1's auc: 0.796082\tvalid_1's binary_logloss: 0.236177\n",
      "##### iteration  2  시작\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chloe/opt/anaconda3/lib/python3.9/site-packages/lightgbm/sklearn.py:598: UserWarning: 'silent' argument is deprecated and will be removed in a future release of LightGBM. Pass 'verbose' parameter via keyword arguments instead.\n",
      "  _log_warning(\"'silent' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "/Users/chloe/opt/anaconda3/lib/python3.9/site-packages/lightgbm/sklearn.py:726: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "/Users/chloe/opt/anaconda3/lib/python3.9/site-packages/lightgbm/sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] num_threads is set with nthread=4, will be overridden by n_jobs=-1. Current value: num_threads=-1\n",
      "[200]\ttraining's auc: 0.795951\ttraining's binary_logloss: 0.238748\tvalid_1's auc: 0.767447\tvalid_1's binary_logloss: 0.245561\n",
      "[400]\ttraining's auc: 0.818591\ttraining's binary_logloss: 0.227523\tvalid_1's auc: 0.780451\tvalid_1's binary_logloss: 0.239515\n",
      "[600]\ttraining's auc: 0.834297\ttraining's binary_logloss: 0.220338\tvalid_1's auc: 0.786381\tvalid_1's binary_logloss: 0.237165\n",
      "[800]\ttraining's auc: 0.847195\ttraining's binary_logloss: 0.214588\tvalid_1's auc: 0.789469\tvalid_1's binary_logloss: 0.236014\n",
      "[1000]\ttraining's auc: 0.858535\ttraining's binary_logloss: 0.209571\tvalid_1's auc: 0.791264\tvalid_1's binary_logloss: 0.235341\n",
      "[1200]\ttraining's auc: 0.869089\ttraining's binary_logloss: 0.204983\tvalid_1's auc: 0.79224\tvalid_1's binary_logloss: 0.234966\n",
      "[1400]\ttraining's auc: 0.878548\ttraining's binary_logloss: 0.200828\tvalid_1's auc: 0.79279\tvalid_1's binary_logloss: 0.23476\n",
      "[1600]\ttraining's auc: 0.887116\ttraining's binary_logloss: 0.196927\tvalid_1's auc: 0.793172\tvalid_1's binary_logloss: 0.234627\n",
      "[1800]\ttraining's auc: 0.894928\ttraining's binary_logloss: 0.193204\tvalid_1's auc: 0.793454\tvalid_1's binary_logloss: 0.234539\n",
      "[2000]\ttraining's auc: 0.902103\ttraining's binary_logloss: 0.189683\tvalid_1's auc: 0.793704\tvalid_1's binary_logloss: 0.234451\n",
      "[2200]\ttraining's auc: 0.908775\ttraining's binary_logloss: 0.186272\tvalid_1's auc: 0.793857\tvalid_1's binary_logloss: 0.234399\n",
      "[2400]\ttraining's auc: 0.914939\ttraining's binary_logloss: 0.183006\tvalid_1's auc: 0.794036\tvalid_1's binary_logloss: 0.234328\n",
      "[2600]\ttraining's auc: 0.920826\ttraining's binary_logloss: 0.179818\tvalid_1's auc: 0.794137\tvalid_1's binary_logloss: 0.234289\n",
      "[2800]\ttraining's auc: 0.926282\ttraining's binary_logloss: 0.176735\tvalid_1's auc: 0.794254\tvalid_1's binary_logloss: 0.23425\n",
      "[3000]\ttraining's auc: 0.931357\ttraining's binary_logloss: 0.173784\tvalid_1's auc: 0.794333\tvalid_1's binary_logloss: 0.234229\n",
      "[3200]\ttraining's auc: 0.93612\ttraining's binary_logloss: 0.170875\tvalid_1's auc: 0.794322\tvalid_1's binary_logloss: 0.234238\n",
      "##### iteration  3  시작\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chloe/opt/anaconda3/lib/python3.9/site-packages/lightgbm/sklearn.py:598: UserWarning: 'silent' argument is deprecated and will be removed in a future release of LightGBM. Pass 'verbose' parameter via keyword arguments instead.\n",
      "  _log_warning(\"'silent' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "/Users/chloe/opt/anaconda3/lib/python3.9/site-packages/lightgbm/sklearn.py:726: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "/Users/chloe/opt/anaconda3/lib/python3.9/site-packages/lightgbm/sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] num_threads is set with nthread=4, will be overridden by n_jobs=-1. Current value: num_threads=-1\n",
      "[200]\ttraining's auc: 0.796126\ttraining's binary_logloss: 0.239066\tvalid_1's auc: 0.76785\tvalid_1's binary_logloss: 0.244089\n",
      "[400]\ttraining's auc: 0.818977\ttraining's binary_logloss: 0.227791\tvalid_1's auc: 0.779377\tvalid_1's binary_logloss: 0.238395\n",
      "[600]\ttraining's auc: 0.834765\ttraining's binary_logloss: 0.220553\tvalid_1's auc: 0.785104\tvalid_1's binary_logloss: 0.236146\n",
      "[800]\ttraining's auc: 0.847441\ttraining's binary_logloss: 0.214864\tvalid_1's auc: 0.788091\tvalid_1's binary_logloss: 0.235038\n",
      "[1000]\ttraining's auc: 0.858766\ttraining's binary_logloss: 0.209862\tvalid_1's auc: 0.790092\tvalid_1's binary_logloss: 0.234383\n",
      "[1200]\ttraining's auc: 0.869126\ttraining's binary_logloss: 0.205288\tvalid_1's auc: 0.791373\tvalid_1's binary_logloss: 0.233986\n",
      "[1400]\ttraining's auc: 0.878164\ttraining's binary_logloss: 0.201164\tvalid_1's auc: 0.792022\tvalid_1's binary_logloss: 0.233788\n",
      "[1600]\ttraining's auc: 0.886557\ttraining's binary_logloss: 0.197308\tvalid_1's auc: 0.792442\tvalid_1's binary_logloss: 0.233657\n",
      "[1800]\ttraining's auc: 0.894213\ttraining's binary_logloss: 0.193632\tvalid_1's auc: 0.792758\tvalid_1's binary_logloss: 0.233559\n",
      "[2000]\ttraining's auc: 0.901267\ttraining's binary_logloss: 0.190132\tvalid_1's auc: 0.792941\tvalid_1's binary_logloss: 0.233503\n",
      "[2200]\ttraining's auc: 0.908104\ttraining's binary_logloss: 0.186732\tvalid_1's auc: 0.792972\tvalid_1's binary_logloss: 0.233494\n",
      "##### iteration  4  시작\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chloe/opt/anaconda3/lib/python3.9/site-packages/lightgbm/sklearn.py:598: UserWarning: 'silent' argument is deprecated and will be removed in a future release of LightGBM. Pass 'verbose' parameter via keyword arguments instead.\n",
      "  _log_warning(\"'silent' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "/Users/chloe/opt/anaconda3/lib/python3.9/site-packages/lightgbm/sklearn.py:726: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "/Users/chloe/opt/anaconda3/lib/python3.9/site-packages/lightgbm/sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] num_threads is set with nthread=4, will be overridden by n_jobs=-1. Current value: num_threads=-1\n",
      "[200]\ttraining's auc: 0.794592\ttraining's binary_logloss: 0.238529\tvalid_1's auc: 0.776128\tvalid_1's binary_logloss: 0.246882\n",
      "[400]\ttraining's auc: 0.817466\ttraining's binary_logloss: 0.227338\tvalid_1's auc: 0.788091\tvalid_1's binary_logloss: 0.240491\n",
      "[600]\ttraining's auc: 0.83323\ttraining's binary_logloss: 0.220193\tvalid_1's auc: 0.793525\tvalid_1's binary_logloss: 0.238021\n",
      "[800]\ttraining's auc: 0.846339\ttraining's binary_logloss: 0.214438\tvalid_1's auc: 0.796525\tvalid_1's binary_logloss: 0.23678\n",
      "[1000]\ttraining's auc: 0.857822\ttraining's binary_logloss: 0.20944\tvalid_1's auc: 0.798319\tvalid_1's binary_logloss: 0.236091\n",
      "[1200]\ttraining's auc: 0.867998\ttraining's binary_logloss: 0.204914\tvalid_1's auc: 0.799561\tvalid_1's binary_logloss: 0.235615\n",
      "[1400]\ttraining's auc: 0.877193\ttraining's binary_logloss: 0.200781\tvalid_1's auc: 0.800343\tvalid_1's binary_logloss: 0.235332\n",
      "[1600]\ttraining's auc: 0.88578\ttraining's binary_logloss: 0.196881\tvalid_1's auc: 0.800948\tvalid_1's binary_logloss: 0.235117\n",
      "[1800]\ttraining's auc: 0.893581\ttraining's binary_logloss: 0.193204\tvalid_1's auc: 0.801374\tvalid_1's binary_logloss: 0.234982\n",
      "[2000]\ttraining's auc: 0.900862\ttraining's binary_logloss: 0.189677\tvalid_1's auc: 0.801664\tvalid_1's binary_logloss: 0.234888\n",
      "[2200]\ttraining's auc: 0.907517\ttraining's binary_logloss: 0.186321\tvalid_1's auc: 0.801836\tvalid_1's binary_logloss: 0.234831\n",
      "[2400]\ttraining's auc: 0.91378\ttraining's binary_logloss: 0.183065\tvalid_1's auc: 0.801955\tvalid_1's binary_logloss: 0.234789\n",
      "[2600]\ttraining's auc: 0.919844\ttraining's binary_logloss: 0.179875\tvalid_1's auc: 0.802096\tvalid_1's binary_logloss: 0.234753\n",
      "[2800]\ttraining's auc: 0.925422\ttraining's binary_logloss: 0.17682\tvalid_1's auc: 0.802123\tvalid_1's binary_logloss: 0.23475\n",
      "2022-08-29 17:09:41.133581\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nPrivate Score: 0.79454, Public Score: 0.80103\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime\n",
    " \n",
    "print(datetime.datetime.now())\n",
    "\n",
    "clf, test_preds = train_apps_all_with_oof(apps_all_train, apps_all_test, nfolds=5)\n",
    "\n",
    "print(datetime.datetime.now())\n",
    "\n",
    "'''\n",
    "Private Score: 0.79454, Public Score: 0.80103\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hqPQR_rhCeXA"
   },
   "outputs": [],
   "source": [
    "apps_all_test['TARGET'] = test_preds\n",
    "apps_all_test[['SK_ID_CURR', 'TARGET']].to_csv('oof_all_02.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ywtBePJUCeXM"
   },
   "outputs": [],
   "source": [
    "apps_all_test = apps_all_test.drop(['TARGET'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jxKitvdGCeXS"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "OOF_All_01.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "fba35677b2d725efedf8456201a17a1e64108c6ceb85b86f6a50c8dc7e15654f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
